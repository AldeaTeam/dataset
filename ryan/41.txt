{"input": "这段文字概述了预测编码(PC)理论及其在机器学习中的实现——预测编码网络(PC networks)。PC理论认为,大脑通过不断生成和优化对外部刺激的预测来学习和编码信息。当预测与实际不符时,大脑会根据预测误差更新突触强度。尽管实验证据不完全一致,但PC理论在计算神经科学研究中影响广泛。机器学习中,PC具有分类和记忆的出色性能,可正向和反向处理信息,这对海马体等大脑区域至关重要。PC还具有深度学习的泛化能力,可在任意神经结构上逼近反向传播,更重要的是,PC只需本地信息即可更新突触,实现完全并行和在任意拓扑结构上训练,而标准深度学习只能单向信息流动并顺序反向传播,无法在循环计算图上训练。因此,PC graphs在任意有向图上实现PC训练。本文采用全连接PC graphs,证明其在不同生成任务上的灵活性。实验还考查不同拓扑结构对PC graphs在生成任务上的影响。","question": "PC理论的核心思想是什么?","<ans>": "通过不断生成和优化对外部刺激的预测来学习和编码信息。"}
{"input": "这段文字概述了预测编码(PC)理论及其在机器学习中的实现——预测编码网络(PC networks)。PC理论认为,大脑通过不断生成和优化对外部刺激的预测来学习和编码信息。当预测与实际不符时,大脑会根据预测误差更新突触强度。尽管实验证据不完全一致,但PC理论在计算神经科学研究中影响广泛。机器学习中,PC具有分类和记忆的出色性能,可正向和反向处理信息,这对海马体等大脑区域至关重要。PC还具有深度学习的泛化能力,可在任意神经结构上逼近反向传播,更重要的是,PC只需本地信息即可更新突触,实现完全并行和在任意拓扑结构上训练,而标准深度学习只能单向信息流动并顺序反向传播,无法在循环计算图上训练。因此,PC graphs在任意有向图上实现PC训练。本文采用全连接PC graphs,证明其在不同生成任务上的灵活性。实验还考查不同拓扑结构对PC graphs在生成任务上的影响。","question": "PC与深度学习相比有什么优点?","<ans>": "PC只需本地信息即可更新突触,实现完全并行和在任意拓扑结构上训练,而标准深度学习只能单向信息流动并顺序反向传播,无法在循环计算图上训练。"}
{"input": "这段文字概述了预测编码(PC)理论及其在机器学习中的实现——预测编码网络(PC networks)。PC理论认为,大脑通过不断生成和优化对外部刺激的预测来学习和编码信息。当预测与实际不符时,大脑会根据预测误差更新突触强度。尽管实验证据不完全一致,但PC理论在计算神经科学研究中影响广泛。机器学习中,PC具有分类和记忆的出色性能,可正向和反向处理信息,这对海马体等大脑区域至关重要。PC还具有深度学习的泛化能力,可在任意神经结构上逼近反向传播,更重要的是,PC只需本地信息即可更新突触,实现完全并行和在任意拓扑结构上训练,而标准深度学习只能单向信息流动并顺序反向传播,无法在循环计算图上训练。因此,PC graphs在任意有向图上实现PC训练。本文采用全连接PC graphs,证明其在不同生成任务上的灵活性。实验还考查不同拓扑结构对PC graphs在生成任务上的影响。","question": "本文在哪种PC graphs上实现了PC训练?","<ans>": "全连接PC graphs"}
{"input": "本段介绍了预测编码网络(PC graphs)的形式化定义。PC graphs是一个有向图G,其中的顶点V分为感知顶点和内部顶点。外部刺激 toujours通过感知顶点呈现,内部顶点表示数据集的内部结构。每个顶点i都编码多个量,主要是随时间变化的活动值节点xi,t。感知顶点的活动值节点称为感知节点。每个顶点还计算其活动的预测μi,t。每个顶点在每个时刻t的误差εi,t定义为其活动值节点与预测之间的差异。活动值节点xi,t和权重参数θi,j更新以最小化每个顶点上局部定义的能量函数Et。PC graphs学习分两阶段:推理和权重更新。推理阶段权重固定,活动值节点连续通过梯度下降更新。权重更新阶段活动值节点固定,执行单次权重更新。文章提出两种查询PC图的内部表示方法:条件查询和初始化查询。在两种情况下,权重参数θi,j现在固定,总能量E通过梯度下降连续最小化活动值节点。条件查询中,一些顶点的活动值节点固定为某些期望的值,未约束的感知顶点会收敛到给定顶点的能量最小值。初始化查询中,一些顶点的活动值节点仅初始化为某些期望的值,每个活动值节点均未约束,并在推理中自由变化。感知顶点会收敛到梯度下降找到的最小值。","question": "PC graphs由哪两类顶点组成?","<ans>": "感知顶点和内部顶点"}
{"input": "本段介绍了预测编码网络(PC graphs)的形式化定义。PC graphs是一个有向图G,其中的顶点V分为感知顶点和内部顶点。外部刺激 toujours通过感知顶点呈现,内部顶点表示数据集的内部结构。每个顶点i都编码多个量,主要是随时间变化的活动值节点xi,t。感知顶点的活动值节点称为感知节点。每个顶点还计算其活动的预测μi,t。每个顶点在每个时刻t的误差εi,t定义为其活动值节点与预测之间的差异。活动值节点xi,t和权重参数θi,j更新以最小化每个顶点上局部定义的能量函数Et。PC graphs学习分两阶段:推理和权重更新。推理阶段权重固定,活动值节点连续通过梯度下降更新。权重更新阶段活动值节点固定,执行单次权重更新。文章提出两种查询PC图的内部表示方法:条件查询和初始化查询。在两种情况下,权重参数θi,j现在固定,总能量E通过梯度下降连续最小化活动值节点。条件查询中,一些顶点的活动值节点固定为某些期望的值,未约束的感知顶点会收敛到给定顶点的能量最小值。初始化查询中,一些顶点的活动值节点仅初始化为某些期望的值,每个活动值节点均未约束,并在推理中自由变化。感知顶点会收敛到梯度下降找到的最小值。","question": "查询PC graphs的两种方式是什么?","<ans>": "条件查询和初始化查询"}
{"input": "本段介绍了预测编码网络(PC graphs)的形式化定义。PC graphs是一个有向图G,其中的顶点V分为感知顶点和内部顶点。外部刺激 toujours通过感知顶点呈现,内部顶点表示数据集的内部结构。每个顶点i都编码多个量,主要是随时间变化的活动值节点xi,t。感知顶点的活动值节点称为感知节点。每个顶点还计算其活动的预测μi,t。每个顶点在每个时刻t的误差εi,t定义为其活动值节点与预测之间的差异。活动值节点xi,t和权重参数θi,j更新以最小化每个顶点上局部定义的能量函数Et。PC graphs学习分两阶段:推理和权重更新。推理阶段权重固定,活动值节点连续通过梯度下降更新。权重更新阶段活动值节点固定,执行单次权重更新。文章提出两种查询PC图的内部表示方法:条件查询和初始化查询。在两种情况下,权重参数θi,j现在固定,总能量E通过梯度下降连续最小化活动值节点。条件查询中,一些顶点的活动值节点固定为某些期望的值,未约束的感知顶点会收敛到给定顶点的能量最小值。初始化查询中,一些顶点的活动值节点仅初始化为某些期望的值,每个活动值节点均未约束,并在推理中自由变化。感知顶点会收敛到梯度下降找到的最小值。","question": "PC graphs的学习分几个阶段?","<ans>": "两个阶段:推理和权重更新"}
{"input": "在这一部分,我们对完全连接的PC图(每个顶点都与其他每个顶点相连)进行实验。这样的PC图是完全普遍的,并不假定数据集的结构。通过简单地修剪G的特定权重,可以获得任何可能的图拓扑结构。给定数据集,我们按照第2节所述训练PC图:第一个d个神经元固定到训练集点的条目,并通过推理和权重更新使能量函数Et最小化,通过等式(3)和(4)。训练完成后,我们展示了可以在不重训练模型的情况下执行的不同任务。我们使用MNIST和FashionMNIST,固定第一个d个节点的数据点,并展示如何通过第2节中描述的方式查询PC图来进行生成、降噪、重构(无标签和有标签)和分类任务。","question": "这部分实验使用的PC图类型是什么?","<ans>": "完全连接的PC图"}
{"input": "在这一部分,我们对完全连接的PC图(每个顶点都与其他每个顶点相连)进行实验。这样的PC图是完全普遍的,并不假定数据集的结构。通过简单地修剪G的特定权重,可以获得任何可能的图拓扑结构。给定数据集,我们按照第2节所述训练PC图:第一个d个神经元固定到训练集点的条目,并通过推理和权重更新使能量函数Et最小化,通过等式(3)和(4)。训练完成后,我们展示了可以在不重训练模型的情况下执行的不同任务。我们使用MNIST和FashionMNIST,固定第一个d个节点的数据点,并展示如何通过第2节中描述的方式查询PC图来进行生成、降噪、重构(无标签和有标签)和分类任务。","question": "实验中如何训练PC图?","<ans>": "按照第2节所述训练PC图:第一个d个神经元固定到训练集点的条目,并通过推理和权重更新使能量函数Et最小化,通过等式(3)和(4)"}
{"input": "在这一部分,我们对完全连接的PC图(每个顶点都与其他每个顶点相连)进行实验。这样的PC图是完全普遍的,并不假定数据集的结构。通过简单地修剪G的特定权重,可以获得任何可能的图拓扑结构。给定数据集,我们按照第2节所述训练PC图:第一个d个神经元固定到训练集点的条目,并通过推理和权重更新使能量函数Et最小化,通过等式(3)和(4)。训练完成后,我们展示了可以在不重训练模型的情况下执行的不同任务。我们使用MNIST和FashionMNIST,固定第一个d个节点的数据点,并展示如何通过第2节中描述的方式查询PC图来进行生成、降噪、重构(无标签和有标签)和分类任务。","question": "PC图能够在不对模型进行重新训练的情况下执行哪些任务?","<ans>": "生成、降噪、重构和分类"}
{"input": "假设我们需要从一幅图像的不完整版本中重构测试图像。如上所述,这可以通过条件查询模型来实现。但是,现在假设我们也提供了损坏图像的标签。能够使用此额外信息获得更好的重构会很有用。在PC图中,这很简单:只需要同时固定表示标签的价值节点到提供的标签的1-hot向量,并将感觉节点固定到损坏图像的像素。我们已经在图2d中显示了这如何改进重构质量。然而,这种方法有一个第二个更重要的应用。有时,很难判断不完整图像属于哪个类别,在重构过程中提供标签允许首选标签影响重构。因此,我们执行以下任务:我们提供在不完整时看起来相似的数字图像,并在提供标签信息时要求模型重构丢失的一半,即使用额外的标签信息来正确解决重构任务中的固有歧义。","question": "如何查询PC图?","<ans>": "通过条件查询"}
{"input": "假设我们需要从一幅图像的不完整版本中重构测试图像。如上所述,这可以通过条件查询模型来实现。但是,现在假设我们也提供了损坏图像的标签。能够使用此额外信息获得更好的重构会很有用。在PC图中,这很简单:只需要同时固定表示标签的价值节点到提供的标签的1-hot向量,并将感觉节点固定到损坏图像的像素。我们已经在图2d中显示了这如何改进重构质量。然而,这种方法有一个第二个更重要的应用。有时,很难判断不完整图像属于哪个类别,在重构过程中提供标签允许首选标签影响重构。因此,我们执行以下任务:我们提供在不完整时看起来相似的数字图像,并在提供标签信息时要求模型重构丢失的一半,即使用额外的标签信息来正确解决重构任务中的固有歧义。","question": "条件查询时模型使用了什么额外的信息?","<ans>": "提供的图像标签"}
{"input": "假设我们需要从一幅图像的不完整版本中重构测试图像。如上所述,这可以通过条件查询模型来实现。但是,现在假设我们也提供了损坏图像的标签。能够使用此额外信息获得更好的重构会很有用。在PC图中,这很简单:只需要同时固定表示标签的价值节点到提供的标签的1-hot向量,并将感觉节点固定到损坏图像的像素。我们已经在图2d中显示了这如何改进重构质量。然而,这种方法有一个第二个更重要的应用。有时,很难判断不完整图像属于哪个类别,在重构过程中提供标签允许首选标签影响重构。因此,我们执行以下任务:我们提供在不完整时看起来相似的数字图像,并在提供标签信息时要求模型重构丢失的一半,即使用额外的标签信息来正确解决重构任务中的固有歧义。","question": "模型利用额外的标签信息来解决什么问题?","<ans>": "重构任务中的固有歧义"}
{"intput": "本段概述了与本文相关的工作。本工作与一整个研究领域共享相似性和最终目标,该研究领域旨在使用计算神经科学的技术改进当前的神经网络。实际上,BP在[39]中强调的生物不合理性和局限性催生了寻找训练ANN的新学习算法的研究,最有希望的候选者是基于能量的模型,如平衡传播[40,41]、玻尔兹曼机[31](包括深度玻尔兹曼机[42]和深度信念网络[43])以及PC[7]。后者是本文的主要研究领域。 PC文献 ranging 从心理学到神经科学和机器学习。特别地,它提供了一个机制,可以解释大脑中观察到的各种知觉现象,例如end-stopping [7]、重复抑制[44]、幻觉运动[45,46]、双稳知觉[47,48],甚至注意力对神经活动的调制[49,50],它甚至被用于描述人类记忆系统中的检索和存储记忆[19]。 尽管受皮层神经科学模型的启发,Rao和Ballard[7]提出的计算模型仍然存在一些不合理的地方,其中主要的一个是对称连接的存在。在[51]中提出了一种PC的实现,没有对称连接,能够成功地学习图像分类任务。尽管与PC无直接关系,但已经证明,ANN不需要对称连接即可在ImageNet等高度复杂的数据集上工作[52,53,54]。最后,有一系列工作专注于建立合理的体系结构,将大脑区域划分为彼此相连的区域[25,26]。在这里,作者使用可塑性和抑制来建立一个描述复杂的认知现象的模型。","question": "当前神经网络存在什么生物不合理性和局限性?","<ans>": "BP在[39]中强调的生物不合理性和局限性"}
{"intput": "本段概述了与本文相关的工作。本工作与一整个研究领域共享相似性和最终目标,该研究领域旨在使用计算神经科学的技术改进当前的神经网络。实际上,BP在[39]中强调的生物不合理性和局限性催生了寻找训练ANN的新学习算法的研究,最有希望的候选者是基于能量的模型,如平衡传播[40,41]、玻尔兹曼机[31](包括深度玻尔兹曼机[42]和深度信念网络[43])以及PC[7]。后者是本文的主要研究领域。 PC文献 ranging 从心理学到神经科学和机器学习。特别地,它提供了一个机制,可以解释大脑中观察到的各种知觉现象,例如end-stopping [7]、重复抑制[44]、幻觉运动[45,46]、双稳知觉[47,48],甚至注意力对神经活动的调制[49,50],它甚至被用于描述人类记忆系统中的检索和存储记忆[19]。 尽管受皮层神经科学模型的启发,Rao和Ballard[7]提出的计算模型仍然存在一些不合理的地方,其中主要的一个是对称连接的存在。在[51]中提出了一种PC的实现,没有对称连接,能够成功地学习图像分类任务。尽管与PC无直接关系,但已经证明,ANN不需要对称连接即可在ImageNet等高度复杂的数据集上工作[52,53,54]。最后,有一系列工作专注于建立合理的体系结构,将大脑区域划分为彼此相连的区域[25,26]。在这里,作者使用可塑性和抑制来建立一个描述复杂的认知现象的模型。","question": "为了找寻训练ANN的新学习算法,最有希望的候选者是什么?","<ans>": "基于能量的模型,如平衡传播[40,41]、玻尔兹曼机[31]以及PC[7]"}
{"intput": "本段概述了与本文相关的工作。本工作与一整个研究领域共享相似性和最终目标,该研究领域旨在使用计算神经科学的技术改进当前的神经网络。实际上,BP在[39]中强调的生物不合理性和局限性催生了寻找训练ANN的新学习算法的研究,最有希望的候选者是基于能量的模型,如平衡传播[40,41]、玻尔兹曼机[31](包括深度玻尔兹曼机[42]和深度信念网络[43])以及PC[7]。后者是本文的主要研究领域。 PC文献 ranging 从心理学到神经科学和机器学习。特别地,它提供了一个机制,可以解释大脑中观察到的各种知觉现象,例如end-stopping [7]、重复抑制[44]、幻觉运动[45,46]、双稳知觉[47,48],甚至注意力对神经活动的调制[49,50],它甚至被用于描述人类记忆系统中的检索和存储记忆[19]。 尽管受皮层神经科学模型的启发,Rao和Ballard[7]提出的计算模型仍然存在一些不合理的地方,其中主要的一个是对称连接的存在。在[51]中提出了一种PC的实现,没有对称连接,能够成功地学习图像分类任务。尽管与PC无直接关系,但已经证明,ANN不需要对称连接即可在ImageNet等高度复杂的数据集上工作[52,53,54]。最后,有一系列工作专注于建立合理的体系结构,将大脑区域划分为彼此相连的区域[25,26]。在这里,作者使用可塑性和抑制来建立一个描述复杂的认知现象的模型。","question": "Rao和Ballard[7]提出的计算模型仍然存在什么不合理的地方?","<ans>": "对称连接的存在"}
{"input": "本段讨论了本文的工作。我们证明了预测编码(PC),最初开发用于仿真视觉皮层的学习,能够在任何拓扑结构的图上执行机器学习任务,称为PC图。特别地,我们强调了我们的框架与标准深度学习之间的两个主要差异:结构的灵活性和查询的灵活性。灵活的结构允许在任何图拓扑上学习,因此包括典型的深度学习模型和类似于稀疏大脑区域的小世界网络。灵活的查询允许模型在携带不同类型信息的数据点上训练和测试:监督信号、无监督和不完整的数据。这项工作引入了一种新的学习框架,当在前馈体系结构上测试时与标准PC的先前工作等价,但它也能够推广到不同的研究方向。例如,它可以用于开发新的神经体系结构搜索算法,这些算法现在不限于F:Rd→Rk表达的层次有向无环图计算图的函数空间,也可以用于学习多模态数据集,其中模态位于不同的大脑区域。另一个研究方向是在处理序列数据时对此模型的应用,这种设置自然适合于一种在离散时间步t上学习并允许任意循环的框架。在更广泛的层面上,这项工作加强了机器学习和神经科学社区之间的联系,因为它强调了PC在这两个领域的重要性,既作为训练受大脑启发的体系结构的高度合理的算法,也作为解决机器智能相关问题的方法。","question": "PC最初开发用于仿真什么?","<ans>": "视觉皮层的学习"}
{"input": "本段讨论了本文的工作。我们证明了预测编码(PC),最初开发用于仿真视觉皮层的学习,能够在任何拓扑结构的图上执行机器学习任务,称为PC图。特别地,我们强调了我们的框架与标准深度学习之间的两个主要差异:结构的灵活性和查询的灵活性。灵活的结构允许在任何图拓扑上学习,因此包括典型的深度学习模型和类似于稀疏大脑区域的小世界网络。灵活的查询允许模型在携带不同类型信息的数据点上训练和测试:监督信号、无监督和不完整的数据。这项工作引入了一种新的学习框架,当在前馈体系结构上测试时与标准PC的先前工作等价,但它也能够推广到不同的研究方向。例如,它可以用于开发新的神经体系结构搜索算法,这些算法现在不限于F:Rd→Rk表达的层次有向无环图计算图的函数空间,也可以用于学习多模态数据集,其中模态位于不同的大脑区域。另一个研究方向是在处理序列数据时对此模型的应用,这种设置自然适合于一种在离散时间步t上学习并允许任意循环的框架。在更广泛的层面上,这项工作加强了机器学习和神经科学社区之间的联系,因为它强调了PC在这两个领域的重要性,既作为训练受大脑启发的体系结构的高度合理的算法,也作为解决机器智能相关问题的方法。","question": "本框架与标准深度学习之间的两个主要差异是什么?","<ans>": "结构的灵活性和查询的灵活性"}
{"input": "本段讨论了本文的工作。我们证明了预测编码(PC),最初开发用于仿真视觉皮层的学习,能够在任何拓扑结构的图上执行机器学习任务,称为PC图。特别地,我们强调了我们的框架与标准深度学习之间的两个主要差异:结构的灵活性和查询的灵活性。灵活的结构允许在任何图拓扑上学习,因此包括典型的深度学习模型和类似于稀疏大脑区域的小世界网络。灵活的查询允许模型在携带不同类型信息的数据点上训练和测试:监督信号、无监督和不完整的数据。这项工作引入了一种新的学习框架,当在前馈体系结构上测试时与标准PC的先前工作等价,但它也能够推广到不同的研究方向。例如,它可以用于开发新的神经体系结构搜索算法,这些算法现在不限于F:Rd→Rk表达的层次有向无环图计算图的函数空间,也可以用于学习多模态数据集,其中模态位于不同的大脑区域。另一个研究方向是在处理序列数据时对此模型的应用,这种设置自然适合于一种在离散时间步t上学习并允许任意循环的框架。在更广泛的层面上,这项工作加强了机器学习和神经科学社区之间的联系,因为它强调了PC在这两个领域的重要性,既作为训练受大脑启发的体系结构的高度合理的算法,也作为解决机器智能相关问题的方法。","question": "这项工作加强了机器学习和哪个社区之间的联系?","<ans>": "神经科学社区"}
{"input": "与反向传播(BP)相比,预测编码(PC)允许更灵活地定义,训练和评估模型。本文报告的实验显示了在特定任务上取得的最佳结果,因此只显示了特定超参数集的效果。 因此,PC中存在的全部可能性尚未显示,然而其他配置可能在其他场景中有帮助。 A.1架构和超参数 在此部分,我们详细描述了在本工作中各种生成任务中获得结果的模型和参数,以保证其可重复性。请注意,我们的目标是比较不同模型的性能,因此我们比较具有相似参数数量的网络。 我们现在简要总结本工作中使用的PC图: • 全连接网络:论文主体的实验通过使用具有2000个顶点的全连接图获得,用于分类和生成任务(784个像素加上10个标签的1-hot矢量),以及784个感知顶点用于重建和消噪。对于彩色图像,我们使用具有5000个顶点的网络。我们训练每个模型20个epoch,并使用early stopping报告最佳结果。作为学习率,我们使用α∈{1,0.5}用于值节点,和η∈{0.0001,0.00005}用于权重,以及权重衰减λ={0.01,0.001, 0.0001,0}。最后,我们使用T = 2000计算每个查询,以确保在达到该值之前能量已收敛。 •前馈网络:由L个完全连接层组成的网络,其维度为H。最佳结果通过L∈{3,4}和H = 512(MNIST)和H = 1024(FashionMNIST)取得。我们未经历增加额外层的任何好处,因为它仅导致更高的收敛时间。相反,宽度直接决定所产生图像的质量:如预期的那样,非常窄的网络无法存储足够的信息来准确重构(或消噪)输入图像。然而,宽网络也表现出次优性能。这是因为更多的参数允许网络轻易过拟合。 结果,生成过程不那么稳定,图像可能出现更加嘈杂,由属于不同类的笔画组成。使用强的权重衰减可以缓解这些问题,我们将后面讨论。 • 循环网络:循环层由其输出通过非线性变换馈送到层输入的层组成。本文使用的循环网络由两个循环层(总共四个非线性变换)组成,当训练MNIST时,其隐藏维度为H = 512,训练FashionMNIST时为H = 1024。给定宽度和深度的选择,表现似乎与前馈网络相似。然而,性能似乎受宽层使用的影响较小。这是由于循环连接建立更多约束,从而产生更高稳定性。 •神经元组装:如论文主体所述,我们使用具有4个3000个顶点簇的模型,以前馈方式连接。作为稀疏性和top-k常数,我们使用p = 0.1和k = 0.2,并执行相同的生成实验。同样,我们训练每个模型20个epoch,并使用early stopping报告最佳结果。作为学习率,我们使用α∈{1,0.5}用于值节点,和η∈{0.0001,0.00005}用于权重。最后,我们使用T = 2000计算每个查询,以确保在达到该值之前能量已收敛。 •自动编码器:自动编码器使用与前馈网络相同的形状定义:它是一个具有L∈{3,4}个隐藏层的全连接网络,其宽度为H∈{256,512,1024}。通过这种方式,结构和参数数量直接对应于使用预测编码训练的前馈网络。它通过BP使用Adam优化器训练,学习率α= 1e-4和权重衰减参数λ∈{1e-2,1e-4,1e-6,0}(最佳结果通过最低值获得)。 由于预测编码需要两组可更新参数,值节点xi,t和权重θi,j,我们定义两个独立的优化器。权重的学习率设置为α= 1e-4,选择的优化器算法是Adam(与自动编码器相同)。我们试验了不同的权重衰减值,注意到最终性能在很大程度上受此值的影响。对于给定的任务,最佳结果通过权重衰减= 1e-2获得。相反,值节点的学习率设置为γ= 1.0,并使用SGD进行优化。最后,我们测试了不同的激活函数;最有希望的似乎是HardTanh。 图7:通过对整个输出层进行条件化进行重建。前馈网络(左)的性能通过使用循环连接(右)显着提高,重构图像不过度拟合噪音,但类似于似乎不错的数字。 图8:使用FashionMNIST样本在MNIST上训练后通过条件化进行重建。前馈网络(左)简单地过度拟合(即在未进行任何修改的情况下重现)输入样本,尽管与训练数据无关。相反,循环网络重现不可识别和阴暗的图像,表明它们不识别输入样本,因为它们不是稳定的数据点。 A.2前馈网络与递归网络 在本工作中,我们强调了在不同情况下,我们可能更喜欢通过条件化或初始化查询。作为规则,条件化意味着我们期望给定网络的部分数据是正确的,并被网络作为存储器无修改地重建。因此,将其用于重建生成任务是有意义的。相反,在进行图像降噪时,我们不希望网络从其存储器中召回嘈杂的图像,而是要求它检索存储器(或生成实际样本),表示与嘈杂输入最接近的合理图像。因此,仅初始化输出层,给网络一个方向遵循,使其不受限制地发展是有意义的。 然而,并不总是清楚哪种查询技术更可取。理想情况下,我们希望网络识别哪些查询数据是真实的(即,与训练样本相似),哪些不是。理想情况下,我们希望网络完美地拟合先前看到的数据点,同时难以重构不熟悉的形状。我们测试了前馈网络和递归网络,方法是在MNIST数据集上训练它们,并通过由一半均匀噪声和一半数字组成的全尺寸图像对输出层进行条件化。结果报告在图7中。我们可以看到前馈网络轻易地拟合噪声,独立地重构两个半份。另一方面,采用循环连接(从而施加更严格的约束)迫使网络作为整体重构图像。 我们可以在图8中看到类似的行为,其中在MNIST上训练的网络用于消除FashionMNIST图像的噪声。前馈网络轻易过度拟合输入样本。相反,递归网络正确地不识别给定的图像,并重构一块无关的和困惑的块。在这最后一种情况下,因此可以通过计算输入图像和输出图像之间的距离来区分熟悉和不熟悉的图像。 A.3权重衰减的重要性 如前所述,权重衰减在确定重构图像的属性方面发挥着基本作用。与其他任务(例如分类)或模型(例如通过BP训练的自动编码器)相比,训练PC时似乎需要更高的值权重衰减。从我们的实验来看,权重衰减阻止网络过度学习它们训练的任务(即在输入时重现任何图像),而是允许它们“理解”每个数据集的几个概念类。这种行为使其可能将其知识推广到新颖和未见的任务,如本文中看到的降噪和重构任务。值得注意的是,当优化单个特定问题(例如图像识别)时,较低的值权重衰减似乎更加有效。 为了显示这一点,我们训练了一个循环网络来通过对输出层的底部进行条件化和在输入中提供目标类标签来重构图像。结果是,在低权重衰减的情况下,网络独立地处理图像的每个半部分,通过拟合条件数据重构底部部分,并使用顶部半部分。 增加权重衰减迫使网络考虑图像作为一个整体,并在两个半部分之间建立联系。 结果,它在没有额外信息(目标类标签)的情况下成功地重构整个数字。这表明,通过增加约束(循环连接和权重衰减),网络在某种程度上“理解”了任务,而不仅仅是学习输入-输出映射。 总之,权重衰减不仅改善生成样本的质量,还提高网络在新任务(未在训练时看到)上的泛化能力。它使网络专注于学习数据集的内在结构和概念,而不仅仅是输入和输出之间的映射,这使其能够推理和在新数据上推断。 虽然权重衰减对预测编码模型特别重要,但其在所有神经网络中都发挥着重要作用。在本工作的上下文中,它增加了模型在重构和消噪任务上的有效性,这些任务涉及到生成新数据而不仅仅是分类或映射输入。相比之下,当训练标准前馈或循环神经网络以进行图像分类时,权重衰减的作用似乎更小。 总之,权重衰减是训练稳定和泛化良好的模型的关键工具。在本工作中,我们强调了它在训练用于生成任务的预测编码模型方面的重要性。然而,这并不是说低的或没有权重衰减就不起作用;相反,需要针对每个模型和任务调整其值以达到最佳性能。","question": "权重衰减在何种意义上对预测编码模型特别重要?","<ans>": "权重衰减对预测编码模型特别重要,因为它增加了模型在重构和消噪任务上的有效性,这些任务涉及生成新数据而不仅仅是分类或映射输入。"}
{"input": "与反向传播(BP)相比,预测编码(PC)允许更灵活地定义,训练和评估模型。本文报告的实验显示了在特定任务上取得的最佳结果,因此只显示了特定超参数集的效果。 因此,PC中存在的全部可能性尚未显示,然而其他配置可能在其他场景中有帮助。 A.1架构和超参数 在此部分,我们详细描述了在本工作中各种生成任务中获得结果的模型和参数,以保证其可重复性。请注意,我们的目标是比较不同模型的性能,因此我们比较具有相似参数数量的网络。 我们现在简要总结本工作中使用的PC图: • 全连接网络:论文主体的实验通过使用具有2000个顶点的全连接图获得,用于分类和生成任务(784个像素加上10个标签的1-hot矢量),以及784个感知顶点用于重建和消噪。对于彩色图像,我们使用具有5000个顶点的网络。我们训练每个模型20个epoch,并使用early stopping报告最佳结果。作为学习率,我们使用α∈{1,0.5}用于值节点,和η∈{0.0001,0.00005}用于权重,以及权重衰减λ={0.01,0.001, 0.0001,0}。最后,我们使用T = 2000计算每个查询,以确保在达到该值之前能量已收敛。 •前馈网络:由L个完全连接层组成的网络,其维度为H。最佳结果通过L∈{3,4}和H = 512(MNIST)和H = 1024(FashionMNIST)取得。我们未经历增加额外层的任何好处,因为它仅导致更高的收敛时间。相反,宽度直接决定所产生图像的质量:如预期的那样,非常窄的网络无法存储足够的信息来准确重构(或消噪)输入图像。然而,宽网络也表现出次优性能。这是因为更多的参数允许网络轻易过拟合。 结果,生成过程不那么稳定,图像可能出现更加嘈杂,由属于不同类的笔画组成。使用强的权重衰减可以缓解这些问题,我们将后面讨论。 • 循环网络:循环层由其输出通过非线性变换馈送到层输入的层组成。本文使用的循环网络由两个循环层(总共四个非线性变换)组成,当训练MNIST时,其隐藏维度为H = 512,训练FashionMNIST时为H = 1024。给定宽度和深度的选择,表现似乎与前馈网络相似。然而,性能似乎受宽层使用的影响较小。这是由于循环连接建立更多约束,从而产生更高稳定性。 •神经元组装:如论文主体所述,我们使用具有4个3000个顶点簇的模型,以前馈方式连接。作为稀疏性和top-k常数,我们使用p = 0.1和k = 0.2,并执行相同的生成实验。同样,我们训练每个模型20个epoch,并使用early stopping报告最佳结果。作为学习率,我们使用α∈{1,0.5}用于值节点,和η∈{0.0001,0.00005}用于权重。最后,我们使用T = 2000计算每个查询,以确保在达到该值之前能量已收敛。 •自动编码器:自动编码器使用与前馈网络相同的形状定义:它是一个具有L∈{3,4}个隐藏层的全连接网络,其宽度为H∈{256,512,1024}。通过这种方式,结构和参数数量直接对应于使用预测编码训练的前馈网络。它通过BP使用Adam优化器训练,学习率α= 1e-4和权重衰减参数λ∈{1e-2,1e-4,1e-6,0}(最佳结果通过最低值获得)。 由于预测编码需要两组可更新参数,值节点xi,t和权重θi,j,我们定义两个独立的优化器。权重的学习率设置为α= 1e-4,选择的优化器算法是Adam(与自动编码器相同)。我们试验了不同的权重衰减值,注意到最终性能在很大程度上受此值的影响。对于给定的任务,最佳结果通过权重衰减= 1e-2获得。相反,值节点的学习率设置为γ= 1.0,并使用SGD进行优化。最后,我们测试了不同的激活函数;最有希望的似乎是HardTanh。 图7:通过对整个输出层进行条件化进行重建。前馈网络(左)的性能通过使用循环连接(右)显着提高,重构图像不过度拟合噪音,但类似于似乎不错的数字。 图8:使用FashionMNIST样本在MNIST上训练后通过条件化进行重建。前馈网络(左)简单地过度拟合(即在未进行任何修改的情况下重现)输入样本,尽管与训练数据无关。相反,循环网络重现不可识别和阴暗的图像,表明它们不识别输入样本,因为它们不是稳定的数据点。 A.2前馈网络与递归网络 在本工作中,我们强调了在不同情况下,我们可能更喜欢通过条件化或初始化查询。作为规则,条件化意味着我们期望给定网络的部分数据是正确的,并被网络作为存储器无修改地重建。因此,将其用于重建生成任务是有意义的。相反,在进行图像降噪时,我们不希望网络从其存储器中召回嘈杂的图像,而是要求它检索存储器(或生成实际样本),表示与嘈杂输入最接近的合理图像。因此,仅初始化输出层,给网络一个方向遵循,使其不受限制地发展是有意义的。 然而,并不总是清楚哪种查询技术更可取。理想情况下,我们希望网络识别哪些查询数据是真实的(即,与训练样本相似),哪些不是。理想情况下,我们希望网络完美地拟合先前看到的数据点,同时难以重构不熟悉的形状。我们测试了前馈网络和递归网络,方法是在MNIST数据集上训练它们,并通过由一半均匀噪声和一半数字组成的全尺寸图像对输出层进行条件化。结果报告在图7中。我们可以看到前馈网络轻易地拟合噪声,独立地重构两个半份。另一方面,采用循环连接(从而施加更严格的约束)迫使网络作为整体重构图像。 我们可以在图8中看到类似的行为,其中在MNIST上训练的网络用于消除FashionMNIST图像的噪声。前馈网络轻易过度拟合输入样本。相反,递归网络正确地不识别给定的图像,并重构一块无关的和困惑的块。在这最后一种情况下,因此可以通过计算输入图像和输出图像之间的距离来区分熟悉和不熟悉的图像。 A.3权重衰减的重要性 如前所述,权重衰减在确定重构图像的属性方面发挥着基本作用。与其他任务(例如分类)或模型(例如通过BP训练的自动编码器)相比,训练PC时似乎需要更高的值权重衰减。从我们的实验来看,权重衰减阻止网络过度学习它们训练的任务(即在输入时重现任何图像),而是允许它们“理解”每个数据集的几个概念类。这种行为使其可能将其知识推广到新颖和未见的任务,如本文中看到的降噪和重构任务。值得注意的是,当优化单个特定问题(例如图像识别)时,较低的值权重衰减似乎更加有效。 为了显示这一点,我们训练了一个循环网络来通过对输出层的底部进行条件化和在输入中提供目标类标签来重构图像。结果是,在低权重衰减的情况下,网络独立地处理图像的每个半部分,通过拟合条件数据重构底部部分,并使用顶部半部分。 增加权重衰减迫使网络考虑图像作为一个整体,并在两个半部分之间建立联系。 结果,它在没有额外信息(目标类标签)的情况下成功地重构整个数字。这表明,通过增加约束(循环连接和权重衰减),网络在某种程度上“理解”了任务,而不仅仅是学习输入-输出映射。 总之,权重衰减不仅改善生成样本的质量,还提高网络在新任务(未在训练时看到)上的泛化能力。它使网络专注于学习数据集的内在结构和概念,而不仅仅是输入和输出之间的映射,这使其能够推理和在新数据上推断。 虽然权重衰减对预测编码模型特别重要,但其在所有神经网络中都发挥着重要作用。在本工作的上下文中,它增加了模型在重构和消噪任务上的有效性,这些任务涉及到生成新数据而不仅仅是分类或映射输入。相比之下,当训练标准前馈或循环神经网络以进行图像分类时,权重衰减的作用似乎更小。 总之,权重衰减是训练稳定和泛化良好的模型的关键工具。在本工作中,我们强调了它在训练用于生成任务的预测编码模型方面的重要性。然而,这并不是说低的或没有权重衰减就不起作用;相反,需要针对每个模型和任务调整其值以达到最佳性能。","question": "循环网络相比前馈网络的优点是什么?","<ans>": "相比于前馈网络,循环网络通过施加更严格的约束来训练网络考虑图像作为一个整体,并在图像的两个半部分之间建立联系。这使循环网络在没有额外信息(如目标类标签)的情况下成功重构整个图像。"}
{"input": "与反向传播(BP)相比,预测编码(PC)允许更灵活地定义,训练和评估模型。本文报告的实验显示了在特定任务上取得的最佳结果,因此只显示了特定超参数集的效果。 因此,PC中存在的全部可能性尚未显示,然而其他配置可能在其他场景中有帮助。 A.1架构和超参数 在此部分,我们详细描述了在本工作中各种生成任务中获得结果的模型和参数,以保证其可重复性。请注意,我们的目标是比较不同模型的性能,因此我们比较具有相似参数数量的网络。 我们现在简要总结本工作中使用的PC图: • 全连接网络:论文主体的实验通过使用具有2000个顶点的全连接图获得,用于分类和生成任务(784个像素加上10个标签的1-hot矢量),以及784个感知顶点用于重建和消噪。对于彩色图像,我们使用具有5000个顶点的网络。我们训练每个模型20个epoch,并使用early stopping报告最佳结果。作为学习率,我们使用α∈{1,0.5}用于值节点,和η∈{0.0001,0.00005}用于权重,以及权重衰减λ={0.01,0.001, 0.0001,0}。最后,我们使用T = 2000计算每个查询,以确保在达到该值之前能量已收敛。 •前馈网络:由L个完全连接层组成的网络,其维度为H。最佳结果通过L∈{3,4}和H = 512(MNIST)和H = 1024(FashionMNIST)取得。我们未经历增加额外层的任何好处,因为它仅导致更高的收敛时间。相反,宽度直接决定所产生图像的质量:如预期的那样,非常窄的网络无法存储足够的信息来准确重构(或消噪)输入图像。然而,宽网络也表现出次优性能。这是因为更多的参数允许网络轻易过拟合。 结果,生成过程不那么稳定,图像可能出现更加嘈杂,由属于不同类的笔画组成。使用强的权重衰减可以缓解这些问题,我们将后面讨论。 • 循环网络:循环层由其输出通过非线性变换馈送到层输入的层组成。本文使用的循环网络由两个循环层(总共四个非线性变换)组成,当训练MNIST时,其隐藏维度为H = 512,训练FashionMNIST时为H = 1024。给定宽度和深度的选择,表现似乎与前馈网络相似。然而,性能似乎受宽层使用的影响较小。这是由于循环连接建立更多约束,从而产生更高稳定性。 •神经元组装:如论文主体所述,我们使用具有4个3000个顶点簇的模型,以前馈方式连接。作为稀疏性和top-k常数,我们使用p = 0.1和k = 0.2,并执行相同的生成实验。同样,我们训练每个模型20个epoch,并使用early stopping报告最佳结果。作为学习率,我们使用α∈{1,0.5}用于值节点,和η∈{0.0001,0.00005}用于权重。最后,我们使用T = 2000计算每个查询,以确保在达到该值之前能量已收敛。 •自动编码器:自动编码器使用与前馈网络相同的形状定义:它是一个具有L∈{3,4}个隐藏层的全连接网络,其宽度为H∈{256,512,1024}。通过这种方式,结构和参数数量直接对应于使用预测编码训练的前馈网络。它通过BP使用Adam优化器训练,学习率α= 1e-4和权重衰减参数λ∈{1e-2,1e-4,1e-6,0}(最佳结果通过最低值获得)。 由于预测编码需要两组可更新参数,值节点xi,t和权重θi,j,我们定义两个独立的优化器。权重的学习率设置为α= 1e-4,选择的优化器算法是Adam(与自动编码器相同)。我们试验了不同的权重衰减值,注意到最终性能在很大程度上受此值的影响。对于给定的任务,最佳结果通过权重衰减= 1e-2获得。相反,值节点的学习率设置为γ= 1.0,并使用SGD进行优化。最后,我们测试了不同的激活函数;最有希望的似乎是HardTanh。 图7:通过对整个输出层进行条件化进行重建。前馈网络(左)的性能通过使用循环连接(右)显着提高,重构图像不过度拟合噪音,但类似于似乎不错的数字。 图8:使用FashionMNIST样本在MNIST上训练后通过条件化进行重建。前馈网络(左)简单地过度拟合(即在未进行任何修改的情况下重现)输入样本,尽管与训练数据无关。相反,循环网络重现不可识别和阴暗的图像,表明它们不识别输入样本,因为它们不是稳定的数据点。 A.2前馈网络与递归网络 在本工作中,我们强调了在不同情况下,我们可能更喜欢通过条件化或初始化查询。作为规则,条件化意味着我们期望给定网络的部分数据是正确的,并被网络作为存储器无修改地重建。因此,将其用于重建生成任务是有意义的。相反,在进行图像降噪时,我们不希望网络从其存储器中召回嘈杂的图像,而是要求它检索存储器(或生成实际样本),表示与嘈杂输入最接近的合理图像。因此,仅初始化输出层,给网络一个方向遵循,使其不受限制地发展是有意义的。 然而,并不总是清楚哪种查询技术更可取。理想情况下,我们希望网络识别哪些查询数据是真实的(即,与训练样本相似),哪些不是。理想情况下,我们希望网络完美地拟合先前看到的数据点,同时难以重构不熟悉的形状。我们测试了前馈网络和递归网络,方法是在MNIST数据集上训练它们,并通过由一半均匀噪声和一半数字组成的全尺寸图像对输出层进行条件化。结果报告在图7中。我们可以看到前馈网络轻易地拟合噪声,独立地重构两个半份。另一方面,采用循环连接(从而施加更严格的约束)迫使网络作为整体重构图像。 我们可以在图8中看到类似的行为,其中在MNIST上训练的网络用于消除FashionMNIST图像的噪声。前馈网络轻易过度拟合输入样本。相反,递归网络正确地不识别给定的图像,并重构一块无关的和困惑的块。在这最后一种情况下,因此可以通过计算输入图像和输出图像之间的距离来区分熟悉和不熟悉的图像。 A.3权重衰减的重要性 如前所述,权重衰减在确定重构图像的属性方面发挥着基本作用。与其他任务(例如分类)或模型(例如通过BP训练的自动编码器)相比,训练PC时似乎需要更高的值权重衰减。从我们的实验来看,权重衰减阻止网络过度学习它们训练的任务(即在输入时重现任何图像),而是允许它们“理解”每个数据集的几个概念类。这种行为使其可能将其知识推广到新颖和未见的任务,如本文中看到的降噪和重构任务。值得注意的是,当优化单个特定问题(例如图像识别)时,较低的值权重衰减似乎更加有效。 为了显示这一点,我们训练了一个循环网络来通过对输出层的底部进行条件化和在输入中提供目标类标签来重构图像。结果是,在低权重衰减的情况下,网络独立地处理图像的每个半部分,通过拟合条件数据重构底部部分,并使用顶部半部分。 增加权重衰减迫使网络考虑图像作为一个整体,并在两个半部分之间建立联系。 结果,它在没有额外信息(目标类标签)的情况下成功地重构整个数字。这表明,通过增加约束(循环连接和权重衰减),网络在某种程度上“理解”了任务,而不仅仅是学习输入-输出映射。 总之,权重衰减不仅改善生成样本的质量,还提高网络在新任务(未在训练时看到)上的泛化能力。它使网络专注于学习数据集的内在结构和概念,而不仅仅是输入和输出之间的映射,这使其能够推理和在新数据上推断。 虽然权重衰减对预测编码模型特别重要,但其在所有神经网络中都发挥着重要作用。在本工作的上下文中,它增加了模型在重构和消噪任务上的有效性,这些任务涉及到生成新数据而不仅仅是分类或映射输入。相比之下,当训练标准前馈或循环神经网络以进行图像分类时,权重衰减的作用似乎更小。 总之,权重衰减是训练稳定和泛化良好的模型的关键工具。在本工作中,我们强调了它在训练用于生成任务的预测编码模型方面的重要性。然而,这并不是说低的或没有权重衰减就不起作用;相反,需要针对每个模型和任务调整其值以达到最佳性能。","question": "权重衰减如何提高网络在新任务上的泛化能力?","<ans>": "权重衰减通过迫使网络专注于学习数据集的内在结构和概念,而不仅仅是输入和输出之间的映射,来提高网络在新任务上的泛化能力。这使网络能推理和在新数据上做出推断。"}
{"input": "关联记忆实验:在论文正文中,我们声称一个全连接的PC图能够进行关联记忆实验。为了证明这一点,我们在CIFAR10和FashionMNIST的不同子集上训练了全连接的PC图,子集的基数分别为{50,100,200}。然后,我们使用初始化查询和条件化查询来检索原始记忆。在这种设置中,如果原训练点与其重构之间的均方误差小于0.001,我们认为记忆已被检索。作为损坏,我们要么移除图像的上半部分,要么用均值为零、方差为0.2的高斯噪声损坏它。结果如图10所示。 结果:实验显示我们的模型能够很好地存储和检索记忆,甚至在彩色图像上进行测试时也是如此。如预期的那样,随着添加更多记忆,重构质量会下降,而随着向模型添加更多参数,重构质量会提高。作为超参数,我们使用η = 0.0001,α = 0.5,T = 5。 分类结果:在论文正文中,我们指出多层PCN在分类任务上与BP的性能相似。这里,我们对此进行了测试,并与文献中的流行模型进行了比较,如受限玻尔兹曼机(RBM)[31]和密集关联记忆(DAM)[55]。总的来说,PCN是唯一能在测试集上与BP表现相似的模型。我们在4个数据集上进行了实验:MNIST、FashionMNIST、SVHN和CIFAR10,结果见表2。 受限玻尔兹曼机:为了全面比较我们的模型与文献中现有模型的生成能力,我们训练了不同的RBM,并进行了重构和消噪任务。结果见图11。特别是,结果显示RBM有时无法检索正确的图像,在消噪中返回一个模糊的点云,并在重构中经常返回相同的图像,即使面对不同的输入也是如此。这个问题在不同批次和RBM的参数中一致出现,而我们提出的任何模型中都没有出现过。 设置:我们训练了几个具有隐藏节点h ∈ {256,512,1024}的RBM,并执行了{1,2,5,10}次吉布斯抽样。我们总是选择最好的结果。","question": "PC图如何证明其能进行关联记忆实验?","<ans>": "为了证明PC图能进行关联记忆实验,作者在CIFAR10和FashionMNIST的不同子集上训练了全连接的PC图。然后,作者使用初始化查询和条件化查询来检索原始记忆。如果原训练点与其重构之间的均方误差小于0.001,作者认为记忆已被检索。"}
{"input": "关联记忆实验:在论文正文中,我们声称一个全连接的PC图能够进行关联记忆实验。为了证明这一点,我们在CIFAR10和FashionMNIST的不同子集上训练了全连接的PC图,子集的基数分别为{50,100,200}。然后,我们使用初始化查询和条件化查询来检索原始记忆。在这种设置中,如果原训练点与其重构之间的均方误差小于0.001,我们认为记忆已被检索。作为损坏,我们要么移除图像的上半部分,要么用均值为零、方差为0.2的高斯噪声损坏它。结果如图10所示。 结果:实验显示我们的模型能够很好地存储和检索记忆,甚至在彩色图像上进行测试时也是如此。如预期的那样,随着添加更多记忆,重构质量会下降,而随着向模型添加更多参数,重构质量会提高。作为超参数,我们使用η = 0.0001,α = 0.5,T = 5。 分类结果:在论文正文中,我们指出多层PCN在分类任务上与BP的性能相似。这里,我们对此进行了测试,并与文献中的流行模型进行了比较,如受限玻尔兹曼机(RBM)[31]和密集关联记忆(DAM)[55]。总的来说,PCN是唯一能在测试集上与BP表现相似的模型。我们在4个数据集上进行了实验:MNIST、FashionMNIST、SVHN和CIFAR10,结果见表2。 受限玻尔兹曼机:为了全面比较我们的模型与文献中现有模型的生成能力,我们训练了不同的RBM,并进行了重构和消噪任务。结果见图11。特别是,结果显示RBM有时无法检索正确的图像,在消噪中返回一个模糊的点云,并在重构中经常返回相同的图像,即使面对不同的输入也是如此。这个问题在不同批次和RBM的参数中一致出现,而我们提出的任何模型中都没有出现过。 设置:我们训练了几个具有隐藏节点h ∈ {256,512,1024}的RBM,并执行了{1,2,5,10}次吉布斯抽样。我们总是选择最好的结果。","question": "随着添加更多记忆和参数,重构质量会发生什么变化?","<ans>": "随着添加更多记忆,重构质量会下降。随着向模型添加更多参数,重构质量会提高。"}
{"input": "关联记忆实验:在论文正文中,我们声称一个全连接的PC图能够进行关联记忆实验。为了证明这一点,我们在CIFAR10和FashionMNIST的不同子集上训练了全连接的PC图,子集的基数分别为{50,100,200}。然后,我们使用初始化查询和条件化查询来检索原始记忆。在这种设置中,如果原训练点与其重构之间的均方误差小于0.001,我们认为记忆已被检索。作为损坏,我们要么移除图像的上半部分,要么用均值为零、方差为0.2的高斯噪声损坏它。结果如图10所示。 结果:实验显示我们的模型能够很好地存储和检索记忆,甚至在彩色图像上进行测试时也是如此。如预期的那样,随着添加更多记忆,重构质量会下降,而随着向模型添加更多参数,重构质量会提高。作为超参数,我们使用η = 0.0001,α = 0.5,T = 5。 分类结果:在论文正文中,我们指出多层PCN在分类任务上与BP的性能相似。这里,我们对此进行了测试,并与文献中的流行模型进行了比较,如受限玻尔兹曼机(RBM)[31]和密集关联记忆(DAM)[55]。总的来说,PCN是唯一能在测试集上与BP表现相似的模型。我们在4个数据集上进行了实验:MNIST、FashionMNIST、SVHN和CIFAR10,结果见表2。 受限玻尔兹曼机:为了全面比较我们的模型与文献中现有模型的生成能力,我们训练了不同的RBM,并进行了重构和消噪任务。结果见图11。特别是,结果显示RBM有时无法检索正确的图像,在消噪中返回一个模糊的点云,并在重构中经常返回相同的图像,即使面对不同的输入也是如此。这个问题在不同批次和RBM的参数中一致出现,而我们提出的任何模型中都没有出现过。 设置:我们训练了几个具有隐藏节点h ∈ {256,512,1024}的RBM,并执行了{1,2,5,10}次吉布斯抽样。我们总是选择最好的结果。","question": "RBM与PCN在重构和消噪任务上的表现有何不同?","<ans>": "RBM有时无法检索正确的图像,在消噪中返回一个模糊的点云,并在重构中经常返回相同的图像,即使面对不同的输入也是如此。相比之下,这些问题在PCN中从未出现过。"}