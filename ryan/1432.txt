{"input":"本文提出了一种名为Back Razor的新型内存高效的迁移学习框架，可以应用于任何预训练网络而无需改变其架构。Back Razor的关键思想是非对称稀疏化：修剪用于反向传播的存储激活，同时保持前向激活的密集性。基于存储激活仅在反向传播时需要的观察，这种非对称修剪避免了影响前向计算精度，从而使更激进的修剪成为可能。此外，我们对Back Razor的收敛速度进行了理论分析，表明在温和条件下，我们的方法保持与纯SGD相似的收敛速度。在卷积神经网络和视觉Transformer上进行了大量的迁移学习实验，包括分类、密集预测和语言建模任务，结果显示Back Razor可以达到高达97%的稀疏度，节省9.2倍的内存使用，而不会丢失准确性。","question":"Back Razor是什么？","<ans>":"Back Razor是一种内存高效的迁移学习框架，可以应用于任何预训练网络而无需改变其架构。"}
{"input":"本文提出了一种名为Back Razor的新型内存高效的迁移学习框架，可以应用于任何预训练网络而无需改变其架构。Back Razor的关键思想是非对称稀疏化：修剪用于反向传播的存储激活，同时保持前向激活的密集性。基于存储激活仅在反向传播时需要的观察，这种非对称修剪避免了影响前向计算精度，从而使更激进的修剪成为可能。此外，我们对Back Razor的收敛速度进行了理论分析，表明在温和条件下，我们的方法保持与纯SGD相似的收敛速度。在卷积神经网络和视觉Transformer上进行了大量的迁移学习实验，包括分类、密集预测和语言建模任务，结果显示Back Razor可以达到高达97%的稀疏度，节省9.2倍的内存使用，而不会丢失准确性。","question":"它的关键思想是什么？","<ans>":"它的关键思想是非对称稀疏化，即修剪用于反向传播的存储激活，同时保持前向激活的密集性。通过观察到存储激活仅在反向传播时需要，Back Razor实现了非对称修剪，避免了影响前向计算精度，从而实现更激进的修剪。"}
{"input":"Back Razor通过非对称稀疏化的方式解决了预训练模型内存占用的问题。它修剪了用于反向传播的存储激活，同时保持前向激活的密集性。通过观察到存储激活仅在反向传播时需要，Back Razor实现了非对称修剪，避免了影响前向计算精度，从而实现更激进的修剪。这种非对称稀疏化的方法有效地减少了预训练模型的内存占用，使其在个人设备上也能够承受得起。","question":"Back Razor是如何解决预训练模型内存占用的问题的？","<ans>":"Back Razor通过修剪用于反向传播的存储激活，同时保持前向激活的密集性来解决预训练模型内存占用的问题。它观察到存储激活仅在反向传播时需要，因此采用非对称稀疏化的方法，只修剪反向传播所需的存储激活，避免了对前向计算精度的影响。这种非对称修剪策略使得更激进的修剪成为可能，从而有效地减少了预训练模型的内存占用。"}
{"input":"这篇论文的背景是关于在边缘设备上进行内存受限的微调问题。边缘设备上部署预训练模型并不足够，因为这些设备可能会持续收集预训练模型从未见过的新数据，并对其进行微调以提高模型性能。然而，边缘设备的内存限制是微调的主要挑战之一。先前的研究发现，用于反向传播的存储激活占据了训练过程中大部分的内存开销。因此，传统的模型压缩技术如权重修剪和量化在减少训练内存开销方面效果不佳。本文提出了一种名为Back Razor的新型内存高效的迁移学习框架，可以直接应用于任何预训练网络而无需改变其架构。Back Razor的关键思想是非对称稀疏化：修剪用于反向传播的存储激活，同时保持前向激活的密集性。通过观察到存储激活仅在反向传播时需要，Back Razor实现了非对称修剪，避免了对前向计算精度的影响，从而实现更激进的修剪。论文还对Back Razor的收敛速度进行了理论分析，表明在温和条件下，该方法保持了与普通随机梯度下降相似的收敛速度。通过在卷积神经网络和Vision Transformer上进行广泛的迁移学习实验，包括分类、密集预测和语言建模任务，论文表明Back Razor可以实现高达97%的稀疏度，节省9.2倍的内存使用，同时不损失准确性。","question":"Back Razor是如何解决预训练模型内存占用的问题的？","<ans>":"Back Razor通过非对称稀疏化的方式解决了预训练模型内存占用的问题。它修剪了用于反向传播的存储激活，同时保持前向激活的密集性。通过观察到存储激活仅在反向传播时需要，Back Razor实现了非对称修剪，避免了对前向计算精度的影响，从而实现更激进的修剪。这种方法可以大幅减少训练过程中的内存开销，使得预训练模型在内存受限的边缘设备上进行微调成为可能。"}
{"input":"这篇论文介绍了一种名为Back Razor的新型内存高效的迁移学习框架。该框架可以应用于任何预训练网络，而无需改变其架构。Back Razor的关键思想是非对称稀疏化，即修剪用于反向传播的存储激活，同时保持前向激活的密集性。存储激活在反向传播时才需要，因此通过非对称修剪可以避免对前向计算精度的影响，从而实现更激进的修剪。论文还对Back Razor的收敛速度进行了理论分析，结果表明在一定条件下，该方法的收敛速度与普通的随机梯度下降（SGD）相似。论文在卷积神经网络和视觉Transformer上进行了广泛的迁移学习实验，涵盖了分类、密集预测和语言建模任务。实验结果显示，Back Razor可以实现高达97%的稀疏度，节省9.2倍的内存使用，同时不损失准确性。","question":"Back Razor如何实现非对称稀疏化来减少内存占用？","<ans>":"Back Razor通过修剪用于反向传播的存储激活，同时保持前向激活的密集性来实现非对称稀疏化。存储激活仅在反向传播时需要，因此通过非对称修剪可以避免对前向计算精度的影响，从而实现更激进的修剪，减少内存占用。"}
{"input":"这篇论文介绍了一种名为Back Razor的新型内存高效的迁移学习框架。该框架可以应用于任何预训练网络，而无需改变其架构。Back Razor的关键思想是非对称稀疏化，即修剪用于反向传播的存储激活，同时保持前向激活的密集性。存储激活在反向传播时才需要，因此通过非对称修剪可以避免对前向计算精度的影响，从而实现更激进的修剪。论文还对Back Razor的收敛速度进行了理论分析，结果表明在一定条件下，该方法的收敛速度与普通的随机梯度下降（SGD）相似。论文在卷积神经网络和视觉Transformer上进行了广泛的迁移学习实验，涵盖了分类、密集预测和语言建模任务。实验结果显示，Back Razor可以实现高达97%的稀疏度，节省9.2倍的内存使用，同时不损失准确性。","question":"Back Razor的关键思想是什么？","<ans>":"Back Razor的关键思想是非对称稀疏化，即修剪用于反向传播的存储激活，同时保持前向激活的密集性。通过非对称修剪，可以避免对前向计算精度的影响，实现更激进的修剪，从而减少内存占用。"}
{"input":"这篇论文介绍了一种名为Back Razor的新型内存高效的迁移学习框架。该框架可以应用于任何预训练网络，而无需改变其架构。Back Razor的关键思想是非对称稀疏化，即修剪用于反向传播的存储激活，同时保持前向激活的密集性。存储激活在反向传播时才需要，因此通过非对称修剪可以避免对前向计算精度的影响，从而实现更激进的修剪。论文还对Back Razor的收敛速度进行了理论分析，结果表明在一定条件下，该方法的收敛速度与普通的随机梯度下降（SGD）相似。论文在卷积神经网络和视觉Transformer上进行了广泛的迁移学习实验，涵盖了分类、密集预测和语言建模任务。实验结果显示，Back Razor可以实现高达97%的稀疏度，节省9.2倍的内存使用，同时不损失准确性。","question":"Back Razor的实验结果表明它可以实现多少的稀疏度和内存节省率？","<ans>":"Back Razor的实验结果表明它可以实现高达97%的稀疏度，节省9.2倍的内存使用。"}